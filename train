#!/usr/bin/env python

import argparse
import torch
from torch import nn
from torch.utils.data import DataLoader
from copy import deepcopy

from data import get_trans, ImageDataset, CLASSES
from models import build_network

parser = argparse.ArgumentParser(description='Simple settings.')
parser.add_argument('--folder', type=str, default='resources/images')
parser.add_argument('--model', type=str, default='tiny', choices=['tiny', 'eff'])
parser.add_argument('--img-size', type=int, default=128)
parser.add_argument('--batch-size', type=int, default=128)
parser.add_argument('--lr', type=float, default=1e-3)
parser.add_argument('--epochs', type=int, default=100)
parser.add_argument('--seed', type=int, default=42)


class Trainer:

    def __init__(self, folder: str, img_size: int = 224, batch_size: int = 32, lr: float = 1e-2):
        # Data loaders, data providers
        self._create_data_loaders(folder, img_size, batch_size)
        # Scaler that scales gradients back to float32 from float16
        self.scaler = torch.cuda.amp.GradScaler()
        # Loss term, categorical_crossentropy
        self.criterion = nn.CrossEntropyLoss()
        # Learning rate
        self.lr = lr

    @classmethod
    def from_args(cls, args):
        '''Create this class from arguments'''
        return cls(args.folder, args.img_size, args.batch_size, args.lr)

    def __call__(self, model: nn.Module, epochs: int = 100):

        # Best measures, saved for test
        best_model, best_loss = None, 1000

        # Define optimizer, where we optimize all parameters of the model
        # that has requires_grad argument enabled (enabled by default)
        self.optimizer = torch.optim.Adam(model.parameters(), lr=self.lr, weight_decay=1e-5)

        # Decrease the learning rate at 60% and 80% of the training process
        milestones = [int(0.6 * epochs), int(0.8 * epochs)]
        self.scheduler = torch.optim.lr_scheduler.MultiStepLR(self.optimizer, milestones=milestones)

        # Run N epochs
        for epoch_i in range(epochs):

            # Tell model where we are in the training so it can
            # adjust its parameters accordingly if it wants to.
            # Often it's unnecessary and equvivalent with doing nothing,
            # but e.g. for transfer learning, we might want to freeze
            # weight for the first 10% of the training, that's where
            # it becomes handy
            model.step(epoch_i / epochs)

            # Train an epoch and validate results
            train_metrics = self._train_one_epoch(model)
            val_metrics = self._validate(model)
            print(
                f"Epoch {epoch_i+1}/{epochs} : " + \
                f"Train loss: {train_metrics['loss']:3.3f} acc: {train_metrics['acc']:2.2f}% |" + \
                f"Val loss: {val_metrics['loss']:3.3f} acc: {val_metrics['acc']:2.2f}%"
            )

            # Step the scheduler, so it knows an epoch is passed and
            # decresases the learning rate in a schedule it was told
            self.scheduler.step()

            # Save best performing model
            if val_metrics['loss'] < best_loss:
                best_loss = val_metrics['loss']
                best_model = deepcopy(model)

        # Evaluate on test
        print("\nTesting:\n")
        test_metrics = self._test(best_model)
        print(f"Test results : Accuracy: {test_metrics['acc']:2.2f}%")

    def _train_one_epoch(self, model: nn.Module) -> None:

        # Metrics for loss and accuracy calculation
        hits, total, loss_sum = 0, 0, 0

        # Put the model in train mode. It means that dropout
        # layers will be active and batch_norm layers will standardize
        # data from the current batch and won't use its
        # running mean or variance.
        model.train()

        # Iterate through the dataset in batches
        for (x, y) in self.train_loader:

            # Move data to cuda
            x = x.to('cuda:0', non_blocking=True)
            y = y.to('cuda:0', non_blocking=True)

            # Perform forward pass in float16, half-precision
            # It requires half as much memory and runs faster
            with torch.cuda.amp.autocast(enabled=True):
                y_hat = model(x)

            # Calculate loss in float32 to get accurate gradients
            with torch.cuda.amp.autocast(enabled=False):
                loss = self.criterion(y_hat.float(), y)

            # Save current loss and accuracy metrics
            # We don't need to save gradients, because we won't
            # update the network based on these calculations,
            # therefore, we set no_grad(). It also saves some GPU RAM
            with torch.no_grad():
                hits += (y_hat.argmax(dim=1) == y).sum()
                loss_sum += loss.item()
                total += len(y)

            # Make sure we have zero gradient everywhere first
            self.optimizer.zero_grad(set_to_none=True)

            # Now, calculate gradients backward based on the loss' derivative
            self.scaler.scale(loss).backward()

            # Update model parameters and scale gradients back to
            # float32
            self.scaler.step(self.optimizer)
            self.scaler.update()

        # Return loss and accuracy
        acc = hits / total * 100
        loss = loss_sum / total
        return {'loss': loss, 'acc': acc}

    def _validate(self, model: nn.Module):
        return self._evaluate(model, self.val_loader)

    def _test(self, model: nn.Module):
        return self._evaluate(model, self.test_loader)

    def _evaluate(self, model: nn.Module, data_loader: DataLoader) -> None:
        '''Evaluate the network'''
        # Metrics for loss and accuracy calculation
        hits, total, loss_sum = 0, 0, 0

        # Put the network in evaluation mode, meaning that no
        # dropout will be applied and the batch_norm layers will
        # use the running metrics
        model.eval()

        # Iterate thrugh the val or test data loader in batches
        for (x, y) in data_loader:

            # Move data to cuda
            x = x.to('cuda', non_blocking=True)
            y = y.to('cuda', non_blocking=True)

            # Because we won't update the network in this phase
            # we run everything without saving activations. We do
            # that with torch.no_grad()
            with torch.no_grad():
                y_hat = model(x)
                hits += (y_hat.argmax(dim=1) == y).sum()
                loss_sum += self.criterion(y_hat, y).item()
                total += len(y)

        # Calculate loss and accuracy
        acc = hits / total * 100
        loss = loss_sum / total
        return {'loss': loss, 'acc': acc}

    def _create_data_loaders(self, folder: str, img_size: int, batch_size: int):
        '''Create train, val and test data loaders for training'''

        train_dataset = ImageDataset(folder, 'train.csv', get_trans(img_size, 'train'))
        val_dataset = ImageDataset(folder, 'val.csv', get_trans(img_size, 'val'))
        val_dataset = ImageDataset(folder, 'test.csv', get_trans(img_size, 'test'))

        common_kwargs = dict(batch_size=batch_size, num_workers=6, pin_memory=True)
        self.train_loader = DataLoader(train_dataset, shuffle=True, **common_kwargs)
        self.val_loader = DataLoader(val_dataset, shuffle=False, **common_kwargs)
        self.test_loader = DataLoader(val_dataset, shuffle=False, **common_kwargs)


if __name__ == "__main__":

    args = parser.parse_args()

    # Create trainer object
    trainer = Trainer.from_args(args)

    # Define neural net
    model = build_network(args.model, len(CLASSES)).cuda()
    print(model)

    # Train the neural net for N epochs
    trainer(model, args.epochs)